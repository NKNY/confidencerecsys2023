{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "project_root = os.environ[\"PROJECT_ROOT\"]\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import src.modules.result_analysis.loading as result_loading\n",
    "import src.modules.result_analysis.model_standardization as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figures_root = os.path.join(project_root, \"latex\", \"figures\")\n",
    "os.makedirs(figures_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=20)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath,amssymb,bm,bbm,lmodern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loglik(df):\n",
    "    return np.log(np.take_along_axis((df[bins_mass_cols].values+1e-6)/(1.+1e-5), (df[\"rating\"]*2-1).astype(int).values[:,None], axis=1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bins_mass_cols = [f\"bins_mass_{x}\" for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 10\n",
    "\n",
    "data_path_templates = {\n",
    "    \"MF_128\": os.path.join(project_root, \"logs\", \"LBD_results\", \"MF_128\", \"MF_128-{}-0\", \"export\"),\n",
    "    \"CMF_128\": os.path.join(project_root, \"logs\", \"LBD_results\", \"CMF_128\", \"CMF_128-{}-0\", \"export\"),\n",
    "    \"OrdRec-UI_512\": os.path.join(project_root, \"logs\", \"LBD_results\", \"OrdRec-UI_512\", \"OrdRec-UI_512-{}-0\", \"export\"),\n",
    "    \"LBDS_512_sum_ab\": os.path.join(project_root, \"logs\", \"LBD_results\", \"LBDS_512_sum_ab\", \"LBDS_512_sum_ab-{}-0\", \"export\"),\n",
    "    \"LBDA_512_sum_ab\": os.path.join(project_root, \"logs\", \"LBD_results\", \"LBDA_512_sum_ab\", \"LBDA_512_sum_ab-{}-0\", \"export\")\n",
    "}\n",
    "\n",
    "print(\"Loading data\")\n",
    "data = {k: [result_loading.path_to_df(v.format(i)) for i in range(NUM_FOLDS)] for k, v in data_path_templates.items()}\n",
    "print(\"Standardising\")\n",
    "confidence_models = {k: [ms.standardise_model(k, df) for df in dfs] for k, dfs in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "alternative = \"less\"\n",
    "metric = {k: [np.sqrt((df[\"err_mean\"]**2).mean()) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(f\"RMSE: (i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MAE\n",
    "metric = {k: [np.mean(np.abs(df[\"err_mean\"])) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "alternative = \"less\"\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(\"MAE\")\n",
    "for m, v in metric.items():\n",
    "    print(f\"{m}: {np.mean(v)} ({np.std(v)})\")\n",
    "print(f\"\\n(i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "metric = {k: [np.mean(df[\"highest_correct\"]) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "alternative = \"greater\"\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(\"Accuracy\")\n",
    "for m, v in metric.items():\n",
    "    print(f\"{m}: {np.mean(v)} ({np.std(v)})\")\n",
    "print(f\"\\n(i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loglik\n",
    "metric = {k: [loglik(df) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "alternative = \"greater\"\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(\"Loglik\")\n",
    "for m, v in metric.items():\n",
    "    print(f\"{m}: {np.mean(v)} ({np.std(v)})\")\n",
    "print(f\"\\n(i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NDCG@3\n",
    "ndcg_fn = lambda x: ndcg_score(x[\"rating\"].values[None,:], x[\"mean\"].values[None,:], k=3) if len(x) > 1 else 1.\n",
    "metric = {k: [np.mean(df.groupby(\"uid\")[[\"rating\", \"mean\"]].apply(ndcg_fn)) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "alternative = \"greater\"\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(\"NDCG@3\")\n",
    "for m, v in metric.items():\n",
    "    print(f\"{m}: {np.mean(v)} ({np.std(v)})\")\n",
    "print(f\"\\n(i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NDCG@10\n",
    "ndcg_fn = lambda x: ndcg_score(x[\"rating\"].values[None,:], x[\"mean\"].values[None,:], k=10) if len(x) > 1 else 1.\n",
    "metric = {k: [np.mean(df.groupby(\"uid\")[[\"rating\", \"mean\"]].apply(ndcg_fn)) for df in dfs] for k, dfs in confidence_models.items()}\n",
    "alternative = \"greater\"\n",
    "keys = list(metric.keys())\n",
    "stat_sign = np.zeros((len(metric), len(metric)))\n",
    "for i, k in enumerate(keys):\n",
    "    for j, k2 in enumerate(keys):\n",
    "        if i == j:\n",
    "            continue\n",
    "        stat_sign[i,j] = ss.wilcoxon(metric[k], metric[k2], alternative=alternative).pvalue\n",
    "print(\"NDCG@10\")\n",
    "for m, v in metric.items():\n",
    "    print(f\"{m}: {np.mean(v)} ({np.std(v)})\")\n",
    "print(f\"\\n(i,j) is p-value for alternative hypothesis that i is {alternative} than j.\")\n",
    "print(pd.DataFrame(stat_sign, index=keys, columns=keys))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Conda (tf210)",
   "language": "python",
   "name": "system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
